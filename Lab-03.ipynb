{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculus for Neural Networks\n",
    "\n",
    "Calculus plays a crucial role in understanding and optimizing neural networks. It provides the mathematical foundation for training algorithms and optimization techniques used in neural network models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Derivatives\n",
    "\n",
    "> Derivatives are a fundamental concept in calculus and are used extensively in neural networks. The derivative of a function represents its rate of change at a particular point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Derivatives in Neural Networks\n",
    "\n",
    "In the context of neural networks, derivatives are used to calculate the gradients of the loss function with respect to the model parameters. These gradients indicate the rate of change of the loss function with respect to each parameter, allowing us to determine the direction and magnitude of the parameter updates during the training process.\n",
    "\n",
    "`By computing the derivatives, we can perform backpropagation`, which is a key algorithm for training neural networks. Backpropagation involves propagating the gradients backwards through the network, updating the parameters layer by layer. This iterative process allows the network to learn from the training data and adjust its parameters to minimize the loss function.\n",
    "\n",
    "Derivatives also enable us to apply the chain rule, which is another important concept in calculus used in neural networks. The chain rule allows us to calculate the derivative of a composite function by breaking it down into smaller functions and applying the chain rule iteratively. In neural networks, `the chain rule is used to compute the gradients of the loss function with respect to the intermediate layers of the network, enabling efficient backpropagation.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Chain Rule\n",
    "\n",
    "> The chain rule is another important concept in calculus that is heavily used in neural networks. It allows us to calculate the derivative of a composite function by breaking it down into smaller functions and applying the chain rule iteratively. In neural networks, the chain rule is used to compute the gradients of the loss function with respect to the intermediate layers of the network, enabling efficient backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Calculus in neural networks](Calculus.gif)\n",
    "Source: https://www.youtube.com/watch?v=hUXpIkhVhSY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Optimization\n",
    "\n",
    "> Optimization techniques, such as gradient descent, are used to find the optimal values for the model parameters that minimize the loss function. Calculus provides the necessary tools to optimize neural networks by computing the gradients and updating the parameters in the direction of steepest descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Gradient Descent\n",
    "\n",
    "Gradient descent is an optimization algorithm commonly used in neural networks to find the optimal values for the model parameters. It relies on the concept of derivatives to compute the gradients of the loss function with respect to the parameters. By iteratively updating the parameters in the direction of steepest descent, gradient descent aims to minimize the loss function and improve the performance of the neural network.\n",
    "\n",
    "There are different variants of gradient descent, including batch gradient descent, stochastic gradient descent, and mini-batch gradient descent. Each variant has its own advantages and trade-offs in terms of computational efficiency and convergence speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the gradient function\n",
    "def gradient(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Gradient function:\n",
    "    Calculates the gradients of the loss function with respect to the weight and bias.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: numpy array, input data\n",
    "    - y: numpy array, actual output\n",
    "    - w: float, weight\n",
    "    - b: float, bias\n",
    "    \n",
    "    Returns:\n",
    "    - float, gradient of weight\n",
    "    - float, gradient of bias\n",
    "    \"\"\"\n",
    "    # Calculate the predicted values\n",
    "    y_pred = w * x + b\n",
    "    \n",
    "    # Calculate the gradients of the loss function with respect to weight and bias\n",
    "    dw = 2 * np.mean((y_pred - y) * x)\n",
    "    db = 2 * np.mean(y_pred - y)\n",
    "    \n",
    "    return dw, db\n",
    "\n",
    "# Define the gradient descent function\n",
    "def gradient_descent(x, y, learning_rate, num_iterations):\n",
    "    \"\"\"\n",
    "    Gradient Descent function:\n",
    "    Updates the weight and bias using gradient descent.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: numpy array, input data\n",
    "    - y: numpy array, actual output\n",
    "    - learning_rate: float, learning rate\n",
    "    - num_iterations: int, number of iterations\n",
    "    \n",
    "    Returns:\n",
    "    - float, final weight\n",
    "    - float, final bias\n",
    "    \"\"\"\n",
    "    # Initialize the weight and bias\n",
    "    w = 0\n",
    "    b = 0\n",
    "    \n",
    "    # Perform gradient descent\n",
    "    for i in range(num_iterations):\n",
    "        # Calculate the gradients\n",
    "        dw, db = gradient(x, y, w, b)\n",
    "        \n",
    "        # Update the weight and bias in the opposite direction of the gradient\n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "    \n",
    "    return w, b\n",
    "\n",
    "# Define the input data\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "# Set the learning rate and number of iterations\n",
    "learning_rate = 0.01\n",
    "num_iterations = 100\n",
    "\n",
    "# Perform gradient descent\n",
    "final_w, final_b = gradient_descent(x, y, learning_rate, num_iterations)\n",
    "\n",
    "# Print the final weight and bias\n",
    "print(\"Final Weight:\", final_w)\n",
    "print(\"Final Bias:\", final_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Conclusion\n",
    "\n",
    "> A solid understanding of calculus is essential for effectively working with neural networks. It enables us to compute derivatives, apply the chain rule, and optimize the model parameters. By leveraging calculus, we can train and fine-tune neural networks to achieve better performance and accuracy."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
