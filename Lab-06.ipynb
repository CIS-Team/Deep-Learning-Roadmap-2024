{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers, Neurons and Activation Functions\n",
    "The building blocks..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Layers VS Neurons in Artificial Neural Networks\n",
    "> What's the difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A layer (each column of circles) is a collection of neurons that are organized together in a specific way. In an ANN, layers are stacked on top of each other to form a network. Each layer in the network performs a specific function, such as processing inputs, extracting features, or generating outputs. The number of layers in an ANN and the arrangement of neurons within each layer can vary depending on the specific architecture and task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://editor.analyticsvidhya.com/uploads/25366Convolutional_Neural_Network_to_identify_the_image_of_a_bird.png)\n",
    "</br>\n",
    "Source: analyticsvidhya.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, a neuron (an individual circle), also known as a node or a perceptron, is an individual computational unit within a layer. Neurons receive input signals from the previous layer or external sources, perform computations on these inputs, and transmit the output to the next layer. Each neuron in a layer is connected to multiple neurons in the previous layer and the subsequent layer, forming a network of interconnected nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Biological Neurons vs Artificial Neurons: How They Work\n",
    "\n",
    "Biological neurons and artificial neurons have different mechanisms of operation. Biological neurons rely on electrical and chemical signals to transmit information, while artificial neurons perform mathematical computations on input signals using weights and activation functions. Understanding these differences is essential for developing effective neural network models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Biological Neurons\n",
    "\n",
    "Biological neurons are the fundamental units of the nervous system in living organisms. They work through a series of steps:\n",
    "\n",
    "1. **Signal Reception**: `Dendrites`, the branching extensions of a neuron, receive signals from other neurons or sensory receptors.\n",
    "\n",
    "2. **Signal Integration**: The received signals are combined in the `cell body` of the neuron. This integration process involves the summation of the incoming signals.\n",
    "\n",
    "3. **Signal Transmission**: If the integrated signal surpasses a certain threshold, an electrical impulse called an action potential is generated. This action potential travels along the `axon`, a long fiber-like structure, towards the `synapses`.\n",
    "\n",
    "4. **Signal Communication**: At the `synapses`, the electrical impulse is converted into a chemical signal. Neurotransmitters are released into the synapse, allowing the signal to be transmitted to the dendrites of the next neuron.\n",
    "\n",
    "5. **Signal Processing**: The process repeats in the subsequent `neurons`, enabling the propagation of signals throughout the nervous system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.researchgate.net/profile/Zhenzhu-Meng/publication/339446790/figure/fig2/AS:862019817320450@1582532948784/A-biological-neuron-in-comparison-to-an-artificial-neural-network-a-human-neuron-b.png)\n",
    "</br>\n",
    "A biological neuron in comparison to an artificial neural network: \n",
    "(a) human neuron; (b) artificial neuron; (c) biological synapse; and (d) ANN synapses.\n",
    "\n",
    "[Source](https://www.researchgate.net/figure/A-biological-neuron-in-comparison-to-an-artificial-neural-network-a-human-neuron-b_fig2_339446790)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Artificial Neurons\n",
    "\n",
    "Artificial neurons, also known as nodes or perceptrons, are mathematical models designed to mimic the behavior of biological neurons. They operate in a different manner:\n",
    "\n",
    "1. **Signal Reception**: Artificial `neurons` receive input signals, typically represented as numerical values, from the previous layer or external sources.\n",
    "\n",
    "2. **Signal Weighting**: Each input signal is multiplied by a corresponding `weight`. These weights determine the importance or contribution of each input to the overall computation.\n",
    "\n",
    "3. **Signal Summation**: The weighted input signals are summed up, usually using a linear combination.\n",
    "\n",
    "4. **Activation Function**: The summed signal is passed through an `activation function`, which introduces non-linearity into the computation. The activation function determines the output of the artificial neuron based on the input signal.\n",
    "\n",
    "5. **Signal Transmission**: The output of the artificial neuron is then transmitted to the next layer or used as the final output of the neural network.\n",
    "\n",
    "6. **Learning**: `Neurons` in a neural network learn by adjusting the `weights` and `biases` associated with their connections. This process, known as `backpropagation`, allows the network to optimize its performance and make accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Neurons: The building blocks\n",
    "\n",
    "Neurons in ANNs are similar to their biological counterparts and have three main components:\n",
    "\n",
    "1. **Input**: Neurons receive input signals from the previous layer or external sources. These inputs are multiplied by corresponding weights and summed up.\n",
    "\n",
    "2. **Activation Function**: The weighted sum of inputs is passed through an activation function, which introduces non-linearity into the network. Activation functions determine the output of a neuron based on the input signal.\n",
    "\n",
    "3. **Output**: The output of a neuron is the result of the activation function applied to the weighted sum of inputs. This output is then transmitted to the next layer of neurons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://machinelearningknowledge.ai/wp-content/uploads/2019/06/Artificial-Neuron-Working.gif)\n",
    "</br>\n",
    "Source: machinelearningknowledge.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Neuron Weights and Bias\n",
    "\n",
    "In artificial neural networks (ANNs), each neuron receives input signals from the previous layer or external sources. These input signals are multiplied by corresponding weights and then summed up. Additionally, a bias term is added to the weighted sum before passing it through an activation function.\n",
    "\n",
    "The mathematical formula for a neuron can be represented as follows:\n",
    "\n",
    "$$\n",
    "z = \\sum_{i=1}^{n} (w_i \\cdot x_i) + b\n",
    "$$\n",
    "\n",
    "$$\n",
    "out = \\text{{activation\\_function}}(z)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $z$ is the weighted sum of the inputs plus the bias term,\n",
    "- $w_i$ represents the weights assigned to each input $x_i$,\n",
    "- $b$ is the bias term,\n",
    "- $out$ is the output of the neuron after passing through the activation function.\n",
    "\n",
    "### *3.1.1. What are they*\n",
    "\n",
    "- **Weights**: The weights $w_i$ assigned to each input $x_i$ signal determine the importance or contribution of that input to the overall computation of the neuron. Larger weights indicate a stronger influence of the corresponding input on the neuron's output. By adjusting the weights during training, the network learns to assign `higher weights to more important features and lower weights to less important ones`'.\n",
    "\n",
    "- **Bias**: The bias term $b$ allows the neuron to adjust its output independently of the input signals. It acts as an offset or `threshold`, determining the neuron's activation level. `A positive bias shifts the activation function to the right, while a negative bias shifts it to the left.` By adjusting the bias, the network can control the overall output of the neuron.\n",
    "\n",
    "Understanding and appropriately setting the weights and bias of neurons is crucial for the performance and effectiveness of an artificial neural network. Improper initialization or incorrect adjustment during training can lead to suboptimal results or even convergence issues.\n",
    "\n",
    "### *3.1.2. How to calculate them*\n",
    "\n",
    "The process of calculating neuron weights and bias involves two main steps:\n",
    "\n",
    "1. **Initialization**: Initially, the weights $w_i$ and bias $b$ of a neuron are `randomly assigned or initialized with small values`. This random initialization helps in breaking symmetry and allows the network to learn different features.\n",
    "\n",
    "2. **Training**: During the training process, the network adjusts the weights $w_i$ and bias $b$ to `minimize the difference between predicted outputs and actual outputs`. This adjustment is done using optimization algorithms, such as gradient descent, which iteratively update the weights $w_i$ and bias $b$ based on the error between predicted and actual outputs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Activation Functions\n",
    "\n",
    "The activation function of a neuron determines its output based on the weighted sum of inputs. It introduces non-linearity into the neural network, enabling it to learn complex patterns and make predictions.\n",
    "\n",
    "Common activation functions include:\n",
    "\n",
    "|       |       |       |\n",
    "|-------|-------|-------|\n",
    "| Step  | Sigmoid | Tanh |\n",
    "| ![Step](https://machinelearningknowledge.ai/wp-content/uploads/2019/08/Step-Activation_Function.gif) | ![Sigmoid](https://machinelearningknowledge.ai/wp-content/uploads/2019/08/Sigmoid-Activation-Function.gif) | ![Tanh](https://machinelearningknowledge.ai/wp-content/uploads/2019/08/Tanh-Activation-Function.gif) |\n",
    "| ReLU  | LReLU | Softmax |\n",
    "| ![ReLU](https://machinelearningknowledge.ai/wp-content/uploads/2019/08/Relu-Activation-Function.gif) | ![LReLU](https://machinelearningknowledge.ai/wp-content/uploads/2019/08/Leaky-Relu-Function.gif) | ![Softmax](https://machinelearningknowledge.ai/wp-content/uploads/2019/08/Softmax-Activation-Function.gif) |\n",
    "\n",
    "Source: machinelearningknowledge.ai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *3.2.1. Step Activation Function*\n",
    "\n",
    "> Simple activation function that outputs a binary value based on a threshold. It returns 0 if the input is less than the threshold, and 1 otherwise. The step function is often used in binary classification problems.\n",
    "\n",
    "The mathematical function can be represented as follows:\n",
    "\n",
    "$$\n",
    "\\text{{step}}(x) = \\begin{cases}\n",
    "1, & \\text{{if }} x \\geq 0 \\\\\n",
    "0, & \\text{{otherwise}}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(x):\n",
    "    \"\"\"\n",
    "    Step function:\n",
    "    - Returns 1 if x >= 0, 0 otherwise.\n",
    "    \"\"\"\n",
    "    return np.where(x >= 0, 1, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *3.2.2. Sigmoid Activation Function*\n",
    "\n",
    "> Often used in the output layer for binary classification problems. It squashes the output between 0 and 1, representing the probability of the positive class. Might lead to vanishing gradient problem if used in hidden layers.\n",
    "\n",
    "The sigmoid mathematical function can be represented as follows:\n",
    "\n",
    "$$\n",
    "\\text{{sigmoid}}(x) = \\frac{1}{{1 + e^{-x}}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid function:\n",
    "    - Returns the sigmoid activation of x.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *3.2.3. Tanh (Hyperbolic Tangent) Activation Function*\n",
    "\n",
    "> Squashes the input values between -1 and 1. It is symmetric around the origin and is useful for capturing non-linear relationships in the data. Might lead to vanishing gradient problem if used in hidden layers.\n",
    "\n",
    "The mathematical representation of the hyperbolic tangent (tanh) function is:\n",
    "\n",
    "$$\n",
    "\\text{tanh}(x) = \\frac{{e^x - e^{-x}}}{{e^x + e^{-x}}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    \"\"\"\n",
    "    Compute the hyperbolic tangent of the input.\n",
    "\n",
    "    Parameters:\n",
    "    - x (float or numpy.ndarray): Input value(s) for which to compute the hyperbolic tangent.\n",
    "\n",
    "    Returns:\n",
    "    - float or numpy.ndarray: Hyperbolic tangent of the input value(s).\n",
    "\n",
    "    Notes:\n",
    "    - The hyperbolic tangent function is defined as (e^x - e^-x) / (e^x + e^-x).\n",
    "    - The function is an element-wise operation when applied to a numpy array.\n",
    "\n",
    "    \"\"\"\n",
    "    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *3.2.4. ReLU (Rectified Linear Unit) Activation Function*\n",
    "\n",
    "> Sets all negative values to zero and keeps positive values unchanged. Can suffer from dying neurons problem \"dying ReLU\".\n",
    "\n",
    "The mathematical representation of the ReLU (Rectified Linear Unit) activation function is:\n",
    "\n",
    "$$\n",
    "\\text{{ReLU}}(x) = \\max(0, x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Rectified Linear Unit (ReLU) function:\n",
    "    - Returns max(0, x).\n",
    "    \"\"\"\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *3.2.5. LReLU (Leaky ReLU) Activation Function*\n",
    "\n",
    "> A variation of the ReLU activation function that allows small negative values instead of setting them to zero. This helps to mitigate the \"dying ReLU\" problem, where neurons can become inactive and stop learning.\n",
    "\n",
    "The mathematical representation of Leaky ReLU (LReLU) is:\n",
    "\n",
    "$$\n",
    "\\text{LReLU}(x) = \\begin{cases}\n",
    "x, & \\text{if } x \\geq 0 \\\\\n",
    "\\alpha x, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is a small positive constant that determines the slope of the function for negative values of $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrelu(x, alpha=0.01):\n",
    "    \"\"\"\n",
    "    Leaky ReLU function:\n",
    "    - Returns max(alpha * x, x).\n",
    "    \"\"\"\n",
    "    return np.where(x >= 0, x, alpha * x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *3.2.6. Softmax Activation Function*\n",
    "\n",
    "> Commonly used in the output layer for multi-class classification problems. It normalizes the outputs into a probability distribution over the classes.\n",
    "\n",
    "The mathematical representation of the softmax activation function is:\n",
    "\n",
    "$$\n",
    "\\text{{softmax}}(x_i) = \\frac{{e^{x_i}}}{{\\sum_{j=1}^{n} e^{x_j}}}\n",
    "$$\n",
    "\n",
    "where $x_i$ represents the input value of the $i\\th$ element, and $n$ is the total number of elements in the input vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Softmax function:\n",
    "    Calculates the softmax activation for each row of the input array x.\n",
    "    \n",
    "    Parameters:\n",
    "    - x (numpy.ndarray): Input array of shape (N, C), where N is the number of samples and C is the number of classes.\n",
    "    \n",
    "    Returns:\n",
    "    - numpy.ndarray: Array of shape (N, C) containing the softmax activations for each sample.\n",
    "    \n",
    "    Notes:\n",
    "    - The softmax activation is calculated as follows:\n",
    "        - Subtract the maximum value of each row of x from x to prevent numerical instability.\n",
    "        - Compute the exponential of each element in the resulting array.\n",
    "        - Divide each element in the resulting array by the sum of all elements in the same row.\n",
    "    \"\"\"\n",
    "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
