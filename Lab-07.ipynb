{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward and Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://machinelearningknowledge.ai/wp-content/uploads/2019/10/Backpropagation.gif)\n",
    "</br>\n",
    "Source: machinelearningknowledge.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Forward Propagation\n",
    "\n",
    "> The process of computing the output of a neural network given an input. It involves passing the input through the network's layers, applying the activation functions, and calculating the final output.\n",
    "\n",
    "The steps involved in forward propagation are as follows:\n",
    "\n",
    "1. **Initialize the input values**: The input values are the features or variables that are fed into the neural network. These values are typically represented as a vector or matrix.\n",
    "\n",
    "2. **Multiply the input values by the weights of the first layer and add the biases**: Each layer in the neural network has weights and biases associated with it. The weights determine the strength of the connections between the neurons, while the biases provide an additional constant term. In forward propagation, the input values are multiplied by the weights of the first layer and the biases are added.\n",
    "\n",
    "3. **Apply the activation function to the result**: After multiplying the input values by the weights and adding the biases, the result is passed through an activation function. The activation function introduces non-linearity into the network and helps in capturing complex patterns in the data.\n",
    "\n",
    "4. **Repeat steps 2 and 3 for each subsequent layer until the final output is obtained**: The process of multiplying the input values by the weights, adding the biases, and applying the activation function is repeated for each subsequent layer in the neural network. This allows the network to learn and extract higher-level features from the input data.\n",
    "\n",
    "The output obtained from forward propagation is then used to calculate the loss or error of the network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid function:\n",
    "    - Returns the sigmoid activation of x.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Perform forward propagation for a neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, numpy array of shape (input_size, m)\n",
    "    parameters -- dictionary containing the parameters of the neural network\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- output of the neural network's forward propagation\n",
    "    cache -- dictionary containing the intermediate values needed for backward propagation\n",
    "    \"\"\"\n",
    "    # Retrieve parameters from the dictionary\n",
    "    W1 = parameters['W1'] # weights matrix of first layer\n",
    "    b1 = parameters['b1'] # bias vector of first layer\n",
    "    W2 = parameters['W2'] # weights matrix of second layer\n",
    "    b2 = parameters['b2'] # bias vector of second layer\n",
    "    \n",
    "    # Implement forward propagation\n",
    "    Z1 = np.dot(W1, X) + b1 \n",
    "    A1 = np.tanh(Z1) \n",
    "    Z2 = np.dot(W2, A1) + b2 \n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    # Cache intermediate values for backward propagation\n",
    "    cache = {\n",
    "        'Z1': Z1,\n",
    "        'A1': A1,\n",
    "        'Z2': Z2,\n",
    "        'A2': A2\n",
    "    }\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Backward Propagation\n",
    "\n",
    "> Also known as backpropagation, is the process of updating the weights and biases of a neural network based on the calculated error. It involves propagating the error backwards through the network and adjusting the parameters to minimize the error.\n",
    "\n",
    "The steps involved in backward propagation are as follows:\n",
    "\n",
    "1. **Calculate the gradient of the loss function with respect to the output**: The loss function measures the discrepancy between the predicted output of the neural network and the actual output. The gradient of the loss function with respect to the output is calculated to determine the direction and magnitude of the error.\n",
    "\n",
    "2. **Propagate the gradient backwards through the layers, calculating the gradients of the loss function with respect to the weights and biases of each layer**: The gradient of the loss function is propagated backwards through the layers of the neural network. This involves calculating the gradients of the loss function with respect to the weights and biases of each layer using the chain rule of calculus.\n",
    "\n",
    "3. **Update the weights and biases of each layer using the calculated gradients and a learning rate**: The gradients calculated in the previous step are used to update the weights and biases of each layer. The learning rate determines the step size of the parameter updates and helps in controlling the convergence of the network.\n",
    "\n",
    "By iteratively performing forward and backward propagation, a neural network can learn to improve its predictions and minimize the error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X, Y, cache, parameters, learning_rate):\n",
    "    \"\"\"\n",
    "    Perform backward propagation for a neural network and update the parameters.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, numpy array of shape (input_size, m)\n",
    "    Y -- true labels, numpy array of shape (output_size, m)\n",
    "    cache -- dictionary containing the intermediate values from forward propagation\n",
    "    parameters -- dictionary containing the parameters of the neural network\n",
    "    learning_rate -- learning rate for parameter updates\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- updated parameters after backward propagation\n",
    "    \"\"\"\n",
    "    # Retrieve parameters from the dictionary\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    # Retrieve intermediate values from the cache\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "\n",
    "    m = X.shape[1]  # Get the number of training examples\n",
    "    \n",
    "    # Calculate loss function gradients using chain rule\n",
    "    dZ2 = A2 - Y # derivative of the loss function with respect to Z2\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T) # derivative of the loss function with respect to W2\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True) # derivative of the loss function with respect to b2\n",
    "    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2)) # derivative of the loss function with respect to Z1\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T) # derivative of the loss function with respect to W1\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True) # derivative of the loss function with respect to b1\n",
    "    \n",
    "\n",
    "    # Update parameters using gradient descent optimization\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    \n",
    "    # Update the parameters dictionary\n",
    "    parameters = {\n",
    "        'W1': W1,\n",
    "        'b1': b1,\n",
    "        'W2': W2,\n",
    "        'b2': b2\n",
    "    }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y):\n",
    "    \"\"\"\n",
    "    Compute the mean squared error cost after a number of iterations.\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    cost -- mean squared error cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]  # number of examples\n",
    "\n",
    "    # Compute the mean squared error\n",
    "    cost = (1./m) * np.sum((A2 - Y)**2)\n",
    "\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's perform forward and back propagation in a simple neural network architecture.\n",
    "\n",
    "![alt text](Misc/simplenn.png)\n",
    "\n",
    "Input Layer: This is the layer where the network takes in the inputs. The number of neurons in this layer is equal to the number of features in your input data.\n",
    "\n",
    "Hidden Layer: This is the layer where the network learns to represent the input data. The number of neurons in this layer can be chosen arbitrarily, but it's often chosen to be a number between the number of input neurons and the number of output neurons.\n",
    "\n",
    "Output Layer: This is the layer where the network outputs its predictions. The number of neurons in this layer is equal to the number of classes you're trying to predict (for classification tasks) or 1 (for regression tasks).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize random parameters for a neural network with 2 input units, 3 hidden units, and 1 output unit\n",
    "np.random.seed(0)\n",
    "parameters = {\n",
    "    'W1': np.random.randn(3, 2),  # 3x2 matrix for weights of the first layer (3 connections for each of the 2 neurons in the input layer)\n",
    "    'b1': np.random.randn(3, 1),  # 3x1 matrix for biases of the first layer (3 neurons in the hidden layer)\n",
    "    'W2': np.random.randn(1, 3),  # 1x3 matrix for weights of the second layer (1 connection for each of the 3 neurons in the hidden layer)\n",
    "    'b2': np.random.randn(1, 1)   # 1x1 matrix for biases of the second layer (1 neuron in the output layer)\n",
    "}\n",
    "\n",
    "# Dummy input data\n",
    "X = np.array([[1, 2], [3, 4]]).T  # 2x2 matrix, transpose to match the shape (input_size, m, where m is the number of examples)\n",
    "\n",
    "# Dummy true labels\n",
    "Y = np.array([[1, 0]])  # 1x2 matrix for true labels\n",
    "\n",
    "# List to store costs\n",
    "costs = []\n",
    "\n",
    "# Number of iterations for training\n",
    "num_iterations = 500\n",
    "\n",
    "# Perform forward and backward propagation for a number of iterations\n",
    "for i in range(num_iterations):\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    parameters = backward_propagation(X, Y, cache, parameters, learning_rate=0.01)\n",
    "    cost = compute_cost(A2, Y)\n",
    "    costs.append(cost)\n",
    "\n",
    "# Plot the cost over time\n",
    "plt.plot(costs)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost over time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Final Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Loss\n",
    "\n",
    "In the context of neural networks, loss refers to a measure of how well the network is performing on a given task. It quantifies the `discrepancy between the predicted output of the network and the actual output`. The goal of training a neural network is to minimize this loss, which indicates that the network is making accurate predictions.\n",
    "\n",
    "There are various types of loss functions that can be used depending on the task at hand. For example, `in classification tasks, the cross-entropy loss is commonly used`, while `in regression tasks, the mean squared error (MSE) loss is often used`. The choice of loss function depends on the specific problem and the desired behavior of the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Gradient\n",
    "The gradient is a mathematical concept that represents the `rate of change of a function at a particular point`. In the context of neural networks, the gradient is used to determine the `direction and magnitude of the error` during the training process.\n",
    "\n",
    "During training, the network calculates the gradient of the loss function with respect to the network's parameters, such as the weights and biases. This gradient provides information about `how the loss function changes as the parameters are adjusted`. By following the gradient, the network can update its parameters in a way that reduces the loss and improves its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Optimizers\n",
    "Responsible for updating the weights and biases of the neural network during training. It determines how the model learns from the training data and adjusts the parameters. The goal of an optimizer is to minimize the loss (or cost) by adjusting the parameters of the model.\n",
    "\n",
    "> Note: The loss function deals with a single data instance, whereas the cost function measures the model's error on a group of objects (it is usually the average of the loss functions for each training example in the dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *3.3.1. How Optimizers Work*\n",
    "\n",
    "Optimizers work by iteratively updating the parameters (weights and biases) of the neural network based on the gradients of the loss function with respect to these parameters. The gradients indicate the direction of steepest ascent or descent in the loss landscape. By following the `negative gradient direction`, the optimizer can find the optimal values for the parameters that minimize the loss.\n",
    "\n",
    "The update rule for the parameters is typically based on the gradient descent algorithm. The general steps of the gradient descent algorithm are as follows:\n",
    "\n",
    "1. **Initialization**: The optimizer initializes the weights and biases of the neural network with small random values.\n",
    "\n",
    "2. **Forward Pass**: The optimizer performs a forward pass through the network to compute the predicted outputs.\n",
    "\n",
    "3. **Loss Calculation**: The optimizer calculates the loss between the predicted outputs and the true labels using a loss function.\n",
    "\n",
    "4. **Backward Pass**: The optimizer performs a backward pass through the network to compute the gradients of the loss with respect to the parameters.\n",
    "\n",
    "5. **Parameter Update**: The optimizer updates the parameters using the gradients and a learning rate. The learning rate determines the step size of the parameter update.\n",
    "\n",
    "6. **Repeat**: Steps 2-5 are repeated for a specified number of iterations or until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://machinelearningknowledge.ai/wp-content/uploads/2019/10/Backpropagation.gif)\n",
    "</br>\n",
    "Source: machinelearningknowledge.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *3.3.2. Different types of Optimizers*\n",
    "\n",
    "There are various types of optimizers available, each with its own advantages and disadvantages. Here are some commonly used optimizers:\n",
    "\n",
    "1. **Stochastic Gradient Descent (SGD)**: SGD is a basic optimization algorithm that updates the parameters based on the gradients of a randomly selected subset of training samples. It is computationally efficient but can be sensitive to the learning rate.\n",
    "\n",
    "2. **Adam**: Adam is an adaptive optimization algorithm that combines the advantages of both AdaGrad and RMSProp. It adapts the learning rate for each parameter based on the estimates of the first and second moments of the gradients. Adam is widely used and performs well in practice.\n",
    "\n",
    "3. **Adagrad**: Adagrad adapts the learning rate for each parameter based on the historical gradients. It performs larger updates for infrequent parameters and smaller updates for frequent parameters. Adagrad is suitable for sparse data and can converge quickly, but its learning rate can become too small over time.\n",
    "\n",
    "4. **RMSProp**: RMSProp is an adaptive optimization algorithm that uses a moving average of the squared gradients to adjust the learning rate. It reduces the learning rate for parameters with large gradients and increases it for parameters with small gradients. RMSProp is effective in handling non-stationary objectives.\n",
    "\n",
    "5. **Momentum**: Momentum adds a fraction of the previous parameter update to the current update. It helps accelerate convergence and overcome local minima. Momentum can be seen as a ball rolling down a hill, gaining momentum as it goes.\n",
    "\n",
    "6. **Nesterov Accelerated Gradient (NAG)**: NAG is an extension of momentum that calculates the gradient at a point slightly ahead in the direction of the momentum. It improves the convergence speed by reducing the oscillations.\n",
    "\n",
    "These are just a few examples of optimizers commonly used in neural network training. Each optimizer has its own characteristics and is suitable for different types of problems. It is important to experiment with different optimizers and tune their hyperparameters to achieve the best performance for a specific task.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
