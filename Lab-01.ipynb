{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks\n",
    "\n",
    "# 1. Overview\n",
    "\n",
    "Neural networks are a type of machine learning model inspired by the structure and function of the human brain. They are widely used in various fields and have shown great success in solving complex problems.\n",
    "\n",
    "In this lab, we will explore the basics of neural networks, their applications, and the typical workflow involved in building and training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Neural Networks\n",
    "\n",
    "A neural network is composed of interconnected nodes, called neurons, organized in layers. Each neuron takes inputs, performs computations, and produces an output. The connections between neurons are represented by weights, which determine the strength of the influence of one neuron on another.\n",
    "\n",
    "Neural networks can be used for various tasks, including classification, regression, and pattern recognition. They have been successfully applied in image and speech recognition, natural language processing, recommendation systems, and many other domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fs3.amazonaws.com%2Fmedia-p.slid.es%2Fuploads%2F359487%2Fimages%2F4362497%2Fezgif-4-843d034145.gif&f=1&nofb=1&ipt=cf136562b09748ddf7d1dacce5ce86443ec7e1c115a9911e4faa31ff58654e38&ipo=images\" width=\"70%\"/>\n",
    "</br>\n",
    "Source: 3Blue1Brown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Workflow\n",
    "\n",
    "The typical workflow for working with neural networks involves the following steps:\n",
    "\n",
    "1. Data Preparation: Collect and preprocess the data for training and testing the neural network.\n",
    "\n",
    "2. Model Architecture: Design the structure of the neural network, including the number of layers, types of activation functions, and the number of neurons in each layer.\n",
    "\n",
    "3. Model Compilation: Configure the model by specifying the loss function, optimizer, and evaluation metrics.\n",
    "\n",
    "4. Model Training: Train the neural network on the training data, adjusting the weights and biases to minimize the loss function.\n",
    "\n",
    "5. Model Evaluation: Evaluate the performance of the trained model on the test data using appropriate metrics.\n",
    "\n",
    "6. Model Deployment: Deploy the trained model to make predictions on new, unseen data.\n",
    "\n",
    "Throughout this lab, we will take a look at each step of the workflow to see what we will be capable of by the end of these labs.\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Data Preparation\n",
    "\n",
    "\tData preparation is an essential step in building and training neural networks. It involves collecting and preprocessing the data to ensure that it is in a suitable format for training and testing the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### *3.1.1. Data Collection*\n",
    "\tThe first step in data preparation is to collect the necessary data for training and testing the neural network. This may involve gathering data from various sources, such as databases, APIs, or files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### *3.1.2. Data Cleaning*\n",
    "\n",
    "\tOnce the data is collected, it is important to clean the data to remove any inconsistencies, errors, or missing values. This may involve techniques such as removing duplicates, handling missing data, and correcting any errors in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- #### *3.1.3. Data Transformation*\n",
    "\n",
    "\tAfter cleaning the data, it may be necessary to transform the data into a format that is suitable for training the neural network. This may involve techniques such as scaling the data, encoding categorical variables, or normalizing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### *3.1.4. Data Splitting*\n",
    "\n",
    "\tTo evaluate the performance of the neural network, it is common to split the data into training and testing sets. The training set is used to train the neural network, while the testing set is used to evaluate its performance. This helps to assess how well the neural network generalizes to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### *3.1.5. Data Augmentation*\n",
    "\n",
    "\tIn some cases, it may be beneficial to augment the data by generating additional training examples. This can help to increase the diversity of the training data and improve the performance of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### *3.1.6. Example*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define the transformations to apply to the dataset\n",
    "transform = transforms.Compose([\n",
    "\ttransforms.RandomRotation(10),  # Apply random rotation\n",
    "\ttransforms.RandomAffine(0, translate=(0.1, 0.1)),  # Apply random affine transformation\n",
    "\ttransforms.ToTensor(),  # Convert the image to a tensor\n",
    "\ttransforms.Normalize((0.5,), (0.5,))  # Normalize the image\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Split the training dataset into training and validation sets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders for training, validation, and testing\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, data preparation is a crucial step in building and training neural networks. It ensures that the data is in a suitable format and quality for training the neural network effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Neural Network Model Architecture\n",
    "\n",
    "\tDesigning the structure of a neural network involves determining the number of layers, the types of activation functions, and the number of neurons in each layer. These choices greatly impact the performance and capabilities of the neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ####  *3.2.1. Number of Layers*\n",
    "\n",
    "\tThe number of layers in a neural network is an important consideration. Generally, neural networks consist of an input layer, one or more hidden layers, and an output layer. The number of hidden layers can vary depending on the complexity of the problem being solved. Adding more layers can potentially improve the network's ability to learn complex patterns, but it also increases the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### *3.2.2. Activation Functions*\n",
    "\n",
    "\tActivation functions introduce non-linearity to the neural network, allowing it to learn and model complex relationships between inputs and outputs. Common activation functions include:\n",
    "\n",
    "\t- **Step Function**: The step function is a simple activation function that outputs a binary value based on a threshold. It returns 0 if the input is less than the threshold, and 1 otherwise. The step function is often used in binary classification problems.\n",
    "\n",
    "\t- **Sigmoid**: Sigmoid is often used in the output layer for binary classification problems. It squashes the output between 0 and 1, representing the probability of the positive class.\n",
    "\n",
    "\t- **Tanh (Hyperbolic Tangent)**: The tanh function is a popular activation function that squashes the input values between -1 and 1. It is symmetric around the origin and is useful for capturing non-linear relationships in the data.\n",
    "\n",
    "\t- **ReLU (Rectified Linear Unit)**: ReLU is a popular choice for hidden layers. It sets all negative values to zero and keeps positive values unchanged.\n",
    "\n",
    "\t- **LReLU (Leaky ReLU)**: The Leaky ReLU is a variation of the ReLU activation function that allows small negative values instead of setting them to zero. This helps to mitigate the \"dying ReLU\" problem, where neurons can become inactive and stop learning.\n",
    "\t\n",
    "\t- **Softmax**: Softmax is commonly used in the output layer for multi-class classification problems. It normalizes the outputs into a probability distribution over the classes.\n",
    "\n",
    "\tThese activation functions can be used in different layers of the neural network depending on the problem and the desired behavior of the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step | Sigmoid | Tanh\n",
    "--- | --- | ---\n",
    "![Step](https://machinelearningknowledge.ai/wp-content/uploads/2019/08/Step-Activation_Function.gif) | ![Sigmoid](https://machinelearningknowledge.ai/wp-content/uploads/2019/08/Sigmoid-Activation-Function.gif) | ![Tanh](https://machinelearningknowledge.ai/wp-content/uploads/2019/08/Tanh-Activation-Function.gif)\n",
    "ReLU | LReLU | Softmax\n",
    "![ReLU](https://machinelearningknowledge.ai/wp-content/uploads/2019/08/Relu-Activation-Function.gif) | ![LReLU](https://machinelearningknowledge.ai/wp-content/uploads/2019/08/Leaky-Relu-Function.gif) | ![Softmax](https://machinelearningknowledge.ai/wp-content/uploads/2019/08/Softmax-Activation-Function.gif)\n",
    "\n",
    "Source: machinelearningknowledge.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### *3.2.3. Number of Neurons in Each Layer*\n",
    "\n",
    "\tThe number of neurons in each layer determines the capacity and complexity of the neural network. The input layer should have the same number of neurons as the number of features in the input data. The number of neurons in the hidden layers can vary depending on the problem complexity and the amount of data available. The output layer should have the same number of neurons as the number of classes in the classification problem.\n",
    "\n",
    "\tIt is important to strike a balance between too few and too many neurons in each layer. Too few neurons may result in underfitting, where the network fails to capture the complexity of the data. Too many neurons may lead to overfitting, where the network memorizes the training data but fails to generalize to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### *3.2.4. Example*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define the neural network architecture\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.sigmoid(out)\n",
    "        out = self.tanh(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Create an instance of the neural network\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 2\n",
    "model = SimpleNet(input_size, hidden_size, output_size)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, designing the neural network model architecture involves carefully considering the number of layers, the types of activation functions, and the number of neurons in each layer. These choices should be based on the problem at hand, the available data, and the desired performance of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Model Compilation\n",
    "\n",
    "\tModel compilation is an important step in building and training neural networks. It involves configuring the model by specifying the loss function, optimizer, and evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### *3.3.1. Loss Function*\n",
    "\n",
    "\tThe loss function measures how well the model performs on the training data. It quantifies the difference between the predicted output and the actual output. The choice of loss function depends on the problem being solved. For example, for binary classification problems, the binary cross-entropy loss function is commonly used. For multi-class classification problems, the categorical cross-entropy loss function is often used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### *3.3.2. Optimizer*\n",
    "\n",
    "\tThe optimizer is responsible for updating the weights and biases of the neural network during training. It determines how the model learns from the training data and adjusts the parameters to minimize the loss function. There are various optimizers available, such as Stochastic Gradient Descent (SGD), Adam, and RMSprop. The choice of optimizer depends on the problem and the characteristics of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### *3.3.3. Evaluation Metrics*\n",
    "\n",
    "\tEvaluation metrics are used to assess the performance of the trained model on the validation or test data. They provide insights into how well the model generalizes to unseen data. Common evaluation metrics include accuracy, precision, recall, and F1 score. The choice of evaluation metrics depends on the problem and the desired performance criteria.\n",
    "\t\n",
    "\tTo compile the model, you need to specify the loss function, optimizer, and evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### *3.3.4 Example*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define the evaluation metric\n",
    "metrics = ['accuracy']\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=loss_fn, optimizer=optimizer, metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Model Training\n",
    "\n",
    "\tModel training is the process of optimizing the neural network's parameters to minimize the loss function and improve its performance on the training data. It involves feeding the training data through the network, computing the loss, and updating the weights and biases using an optimization algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Training NN](https://machinelearningknowledge.ai/wp-content/uploads/2019/10/Backpropagation.gif)\n",
    "</br>\n",
    "Source: machinelearningknowledge.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The steps involved in model training are as follows:\n",
    "\n",
    "\t1. **Forward Pass**: The input data is fed through the network, and the output is computed using the current weights and biases.\n",
    "\n",
    "\t2. **Loss Computation**: The loss function is used to measure the difference between the predicted output and the actual output.\n",
    "\n",
    "\t3. **Backward Pass**: The gradients of the loss with respect to the network's parameters are computed using the chain rule of calculus.\n",
    "\n",
    "\t4. **Parameter Update**: The weights and biases are updated using an optimization algorithm, such as Stochastic Gradient Descent (SGD), Adam, or RMSprop. The update rule adjusts the parameters in the direction that reduces the loss.\n",
    "\n",
    "\t5. **Repeat**: Steps 1-4 are repeated for multiple iterations or epochs until the model converges or reaches a predefined stopping criterion.\n",
    "\n",
    "During model training, it is common to monitor the loss and evaluation metrics on a validation set to assess the model's performance and prevent overfitting. The validation set provides an estimate of how well the model generalizes to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### *3.4.1. Model Validation*\n",
    "\n",
    "\tModel validation is an essential step in the model training process. It involves evaluating the trained model's performance on a separate validation dataset that was not used during training. The validation dataset provides an unbiased estimate of the model's performance on unseen data.\n",
    "\n",
    "\tThe steps involved in model validation are as follows:\n",
    "\n",
    "\t1. **Forward Pass**: The input data from the validation dataset is fed through the trained network, and the output is computed.\n",
    "\n",
    "\t2. **Loss Computation**: The loss function is used to measure the difference between the predicted output and the actual output.\n",
    "\n",
    "\t3. **Evaluation Metrics**: Additional evaluation metrics, such as accuracy, precision, recall, or F1 score, can be computed to assess the model's performance on the validation dataset.\n",
    "\n",
    "\t4. **Repeat**: Steps 1-3 are repeated for all samples in the validation dataset.\n",
    "\n",
    "\tThe validation results provide insights into how well the trained model generalizes to unseen data and can help identify potential issues, such as overfitting or underfitting. Based on the validation results, adjustments can be made to the model architecture, hyperparameters, or training process to improve performance.\n",
    "\n",
    "By regularly evaluating the model on a validation dataset, you can monitor its progress during training and make informed decisions to optimize its performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### *3.4.2 Example*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# Define the number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Iterate over the training dataset\n",
    "    for inputs, targets in train_loader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the model parameters\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evaluate the model on the validation dataset\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_accuracy = 0.0\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            val_loss += loss_fn(outputs, targets).item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_accuracy += (predicted == targets).sum().item()\n",
    "        \n",
    "        val_loss /= len(val_dataset)\n",
    "        val_accuracy /= len(val_dataset)\n",
    "    \n",
    "    # Print the epoch number, loss, and accuracy\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Loss: {val_loss:.4f}, Accuracy: {val_accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Model Evaluation\n",
    "\n",
    "\tModel evaluation is a crucial step in assessing the performance of a trained neural network. It involves measuring how well the model generalizes to unseen data and how accurately it predicts the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various evaluation metrics that can be used to assess the model's performance, depending on the nature of the problem. Some commonly used evaluation metrics include:\n",
    "\n",
    "- **Accuracy**: Accuracy measures the proportion of correctly classified instances out of the total number of instances. It is commonly used for classification problems.\n",
    "\n",
    "- **Precision**: Precision measures the proportion of true positive predictions out of the total number of positive predictions. It is useful when the cost of false positives is high.\n",
    "\n",
    "- **Recall**: Recall measures the proportion of true positive predictions out of the total number of actual positive instances. It is useful when the cost of false negatives is high.\n",
    "\n",
    "- **F1 Score**: The F1 score is the harmonic mean of precision and recall. It provides a balanced measure of the model's performance.\n",
    "\n",
    "- **Mean Squared Error (MSE)**: MSE measures the average squared difference between the predicted and actual values. It is commonly used for regression problems.\n",
    "\n",
    "- **Root Mean Squared Error (RMSE)**: RMSE is the square root of the MSE. It provides a measure of the average magnitude of the prediction errors.\n",
    "\n",
    "- **R-squared (R2) Score**: R2 score measures the proportion of the variance in the target variable that is predictable from the input variables. It ranges from 0 to 1, with 1 indicating a perfect fit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### *3.5.1 Example*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, r2_score\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Create empty lists to store the predicted outputs and actual targets\n",
    "predicted_outputs = []\n",
    "actual_targets = []\n",
    "\n",
    "# Iterate over the test dataloader\n",
    "for inputs, targets in test_loader:\n",
    "\t# Forward pass\n",
    "\toutputs = model(inputs)\n",
    "\t\n",
    "\t# Convert the predicted outputs and actual targets to numpy arrays\n",
    "\tpredicted_outputs.extend(outputs.detach().numpy())\n",
    "\tactual_targets.extend(targets.numpy())\n",
    "\n",
    "# Convert the predicted outputs and actual targets to numpy arrays\n",
    "predicted_outputs = np.array(predicted_outputs)\n",
    "actual_targets = np.array(actual_targets)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(actual_targets, predicted_outputs.argmax(axis=1))\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(actual_targets, predicted_outputs.argmax(axis=1), average='weighted')\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(actual_targets, predicted_outputs.argmax(axis=1), average='weighted')\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(actual_targets, predicted_outputs.argmax(axis=1), average='weighted')\n",
    "\n",
    "# Calculate mean squared error (MSE)\n",
    "mse = mean_squared_error(actual_targets, predicted_outputs)\n",
    "\n",
    "# Calculate root mean squared error (RMSE)\n",
    "rmse = mean_squared_error(actual_targets, predicted_outputs, squared=False)\n",
    "\n",
    "# Calculate R2 score\n",
    "r2 = r2_score(actual_targets, predicted_outputs)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R2 Score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model, the evaluation metrics are computed using the model's predictions and the corresponding ground truth values. The choice of evaluation metrics depends on the problem and the desired performance criteria.\n",
    "\n",
    "It is important to note that model evaluation should be performed on a separate test dataset that was not used during training or model validation. This ensures an unbiased assessment of the model's performance on unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6. Model Deployment\n",
    "\n",
    "\tModel deployment is the process of making a trained model available for use in a production environment. It involves taking the trained model and integrating it into a system or application where it can be used to make predictions on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several approaches to model deployment, depending on the specific requirements and constraints of the deployment environment. Some common methods include:\n",
    "\n",
    "- **API-based Deployment**: In this approach, the trained model is deployed as a web service with an API (Application Programming Interface) that allows other applications to make predictions using the model. This enables easy integration with other systems and applications.\n",
    "\n",
    "- **Containerization**: Containerization involves packaging the trained model and its dependencies into a container, such as a Docker container. This allows for easy deployment and scalability, as the container can be run on any system that supports containerization.\n",
    "\n",
    "- **Serverless Deployment**: Serverless deployment involves deploying the trained model as a function or service on a serverless computing platform, such as AWS Lambda or Azure Functions. This eliminates the need to manage infrastructure and provides automatic scaling.\n",
    "\n",
    "- **Edge Deployment**: In edge deployment, the trained model is deployed on edge devices, such as IoT devices or edge servers, to enable real-time predictions at the edge of the network. This is useful in scenarios where low latency or offline prediction capabilities are required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### *3.6.1 Example*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch\n",
    "\n",
    "# Assuming you have a trained PyTorch model called 'model'\n",
    "\n",
    "# Save the model state to a file for later use\n",
    "checkpoint = {\n",
    "              'state_dict': model.state_dict(),\n",
    "              'class_to_idx': model.class_to_idx\n",
    "             }\n",
    "\n",
    "torch.save(checkpoint, 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the file\n",
    "loaded_model = YourModelClass() # Create an instance of the model class (in our case SimpleNet)\n",
    "checkpoint = torch.load('checkpoint.pth') # Load the checkpoint\n",
    "\n",
    "model.load_state_dict(checkpoint['state_dict']) # Load the model state\n",
    "model.class_to_idx = checkpoint['class_to_idx'] # Load the class to index mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def predict_single_input(model, input_image, topk=1):\n",
    "\t# Preprocess the input image\n",
    "\ttransform = transforms.Compose([\n",
    "\t\ttransforms.ToTensor(),\n",
    "\t\ttransforms.Normalize((0.1307,), (0.3081,))\n",
    "\t])\n",
    "\tinput_tensor = transform(input_image).unsqueeze(0)\n",
    "\t\n",
    "\t# Set the model to evaluation mode\n",
    "\tmodel.eval()\n",
    "\t\n",
    "\t# Disable gradient calculation\n",
    "\twith torch.no_grad():\n",
    "\t\t# Make the prediction\n",
    "\t\toutput = model(input_tensor)\n",
    "\t\tprobabilities = torch.softmax(output, dim=1)[0]\n",
    "\t\ttopk_probabilities, topk_indices = torch.topk(probabilities, k=topk)\n",
    "\t\t\n",
    "\t\t# Get the class labels and probabilities\n",
    "\t\tclass_names = [model.class_to_idx[idx] for idx in topk_indices]\n",
    "\t\tclass_probabilities = topk_probabilities.tolist()\n",
    "\t\n",
    "\t# Return the class names and probabilities\n",
    "\treturn class_names, class_probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When deploying a model, it is important to consider factors such as scalability, security, performance, and monitoring. Additionally, versioning and updating mechanisms should be put in place to ensure that the deployed model can be easily updated or rolled back when necessary.\n",
    "\n",
    "Overall, model deployment is a critical step in the machine learning lifecycle, as it allows the trained model to be used in real-world applications and provides value to end-users."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
