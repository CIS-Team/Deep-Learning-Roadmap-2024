{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Activation Functions\n",
    "\n",
    "> Activation functions introduce non-linearity to the neural network. Common activation functions include sigmoid, tanh, and ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step function\n",
    "def step(x):\n",
    "    \"\"\"\n",
    "    Step function:\n",
    "    - Returns 1 if x >= 0, 0 otherwise.\n",
    "    \"\"\"\n",
    "    return np.where(x >= 0, 1, 0)\n",
    "\n",
    "# Tanh function\n",
    "def tanh(x):\n",
    "    \"\"\"\n",
    "    Hyperbolic tangent (tanh) function:\n",
    "    - Returns the hyperbolic tangent of x.\n",
    "    \"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid function:\n",
    "    - Returns the sigmoid activation of x.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Softmax function\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Softmax function:\n",
    "    - Returns the softmax activation of x.\n",
    "    \"\"\"\n",
    "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "# ReLU function\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Rectified Linear Unit (ReLU) function:\n",
    "    - Returns max(0, x).\n",
    "    \"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Leaky ReLU function\n",
    "def lrelu(x, alpha=0.01):\n",
    "    \"\"\"\n",
    "    Leaky ReLU function:\n",
    "    - Returns max(alpha * x, x).\n",
    "    \"\"\"\n",
    "    return np.where(x >= 0, x, alpha * x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loss Functions\n",
    "\n",
    "> Loss functions measure the error between the predicted output and the actual output. They are used to optimize the weights and biases of the neural network during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "\t\"\"\"\n",
    "\tMean Squared Error (MSE) loss function:\n",
    "\tCalculates the mean squared difference between the true and predicted values.\n",
    "\t\n",
    "\tParameters:\n",
    "\t- y_true: numpy array, true values\n",
    "\t- y_pred: numpy array, predicted values\n",
    "\t\n",
    "\tReturns:\n",
    "\t- float, average squared error\n",
    "\t\"\"\"\n",
    "\treturn np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def bce_loss(y_true, y_pred):\n",
    "\t\"\"\"\n",
    "\tBinary Cross Entropy (BCE) loss function:\n",
    "\tCalculates the binary cross entropy between the true and predicted values.\n",
    "\t\n",
    "\tParameters:\n",
    "\t- y_true: numpy array, true values\n",
    "\t- y_pred: numpy array, predicted values\n",
    "\t\n",
    "\tReturns:\n",
    "\t- float, average cross entropy\n",
    "\t\"\"\"\n",
    "\treturn -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def cce_loss(y_true, y_pred):\n",
    "\t\"\"\"\n",
    "\tCategorical Cross Entropy (CCE) loss function:\n",
    "\tCalculates the categorical cross entropy between the true and predicted values.\n",
    "\t\n",
    "\tParameters:\n",
    "\t- y_true: numpy array, true values\n",
    "\t- y_pred: numpy array, predicted values\n",
    "\t\n",
    "\tReturns:\n",
    "\t- float, average cross entropy\n",
    "\t\"\"\"\n",
    "\treturn -np.mean(y_true * np.log(y_pred))\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "\t\"\"\"\n",
    "\tCustom loss function:\n",
    "\tDefine your own loss function here.\n",
    "\t\n",
    "\tParameters:\n",
    "\t- y_true: numpy array, true values\n",
    "\t- y_pred: numpy array, predicted values\n",
    "\t\n",
    "\tReturns:\n",
    "\t- float, loss value\n",
    "\t\"\"\"\n",
    "\t# Calculate the loss using your own formula\n",
    "\tloss = ...\n",
    "\n",
    "\treturn loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent\n",
    "\n",
    "> Gradient descent is an optimization algorithm used to minimize the loss function. It adjusts the weights and biases of the neural network based on the gradients calculated during backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the gradient function\n",
    "def gradient(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Gradient function:\n",
    "    Calculates the gradients of the loss function with respect to the weight and bias.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: numpy array, input data\n",
    "    - y: numpy array, actual output\n",
    "    - w: float, weight\n",
    "    - b: float, bias\n",
    "    \n",
    "    Returns:\n",
    "    - float, gradient of weight\n",
    "    - float, gradient of bias\n",
    "    \"\"\"\n",
    "    # Calculate the predicted values\n",
    "    y_pred = w * x + b\n",
    "    \n",
    "    # Calculate the gradients\n",
    "    dw = 2 * np.mean((y_pred - y) * x)\n",
    "    db = 2 * np.mean(y_pred - y)\n",
    "    \n",
    "    return dw, db\n",
    "\n",
    "# Define the gradient descent function\n",
    "def gradient_descent(x, y, learning_rate, num_iterations):\n",
    "    \"\"\"\n",
    "    Gradient Descent function:\n",
    "    Updates the weight and bias using gradient descent.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: numpy array, input data\n",
    "    - y: numpy array, actual output\n",
    "    - learning_rate: float, learning rate\n",
    "    - num_iterations: int, number of iterations\n",
    "    \n",
    "    Returns:\n",
    "    - float, final weight\n",
    "    - float, final bias\n",
    "    \"\"\"\n",
    "    # Initialize the weight and bias\n",
    "    w = 0\n",
    "    b = 0\n",
    "    \n",
    "    # Perform gradient descent\n",
    "    for i in range(num_iterations):\n",
    "        # Calculate the gradients\n",
    "        dw, db = gradient(x, y, w, b)\n",
    "        \n",
    "        # Update the weight and bias\n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "    \n",
    "    return w, b\n",
    "\n",
    "# Define the input data\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "# Set the learning rate and number of iterations\n",
    "learning_rate = 0.01\n",
    "num_iterations = 100\n",
    "\n",
    "# Perform gradient descent\n",
    "final_w, final_b = gradient_descent(x, y, learning_rate, num_iterations)\n",
    "\n",
    "# Print the final weight and bias\n",
    "print(\"Final Weight:\", final_w)\n",
    "print(\"Final Bias:\", final_b)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
